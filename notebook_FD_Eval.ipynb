{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook-FD-Eval.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AvRA8rWL9-ln",
        "6pa4UNg9W9TH"
      ],
      "toc_visible": true,
      "mount_file_id": "1KG3Rn8QI1Fylm-uLsi-SuJLkwkwPLAlO",
      "authorship_tag": "ABX9TyNMG+9+u/sNGxoqPvZZ1JUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatehOurghi/face-detection-evaluation/blob/main/notebook_FD_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvRA8rWL9-ln"
      },
      "source": [
        "# Evaluation of face detection algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "This colab notebook will be used to evaluate different face detection algorithms including:\n",
        "1.   TinaFace (best of Wider Face)\n",
        "2.   DSFD (best of FDDB)\n",
        "3.   SRN (best of Pascal Face)\n",
        "---\n",
        "Datasets used in this evaluation are:\n",
        "*   Wider Face\n",
        "*   FDDB\n",
        "*   Pascal Face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvhoHxbb0eu7",
        "outputId": "30574219-07b5-4b28-a1ce-91b34e165d45"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zK8UYp9Hplf"
      },
      "source": [
        "**Machine configurations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeMYPWVjHvvQ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBh1etidm0Wv"
      },
      "source": [
        "# Prepare algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lbmDidKbnFr6"
      },
      "source": [
        "#@title Imports and global constants\n",
        "# install face_detection framework\n",
        "!pip install face-detection\n",
        "!pip install utils\n",
        "!pip install yapf\n",
        "# imports\n",
        "import cv2\n",
        "import face_detection\n",
        "import time\n",
        "import os,sys\n",
        "import os.path as osp\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from utils import *\n",
        "from google.colab.patches import cv2_imshow\n",
        "import colorsys\n",
        "import tensorflow.python.keras.backend as K\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from timeit import default_timer as timer\n",
        "from PIL import ImageDraw, Image\n",
        "import tensorflow as tf\n",
        "\n",
        "ROOT = '/content/drive/MyDrive/Datasets/FaceDetection/'\n",
        "EVAL_FOLDER = \"/content/drive/MyDrive/Evaluations/\"\n",
        "MODEL_FOLDER = \"/content/drive/MyDrive/Models/FaceDetection/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MK4fz_fYnLzH"
      },
      "source": [
        "#@title DSFD detector\n",
        "# prepare detection and evaluation for DSFD\n",
        "def dsfd_detector_eval(annotations, images_folder):\n",
        "  # prepare detector\n",
        "  detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
        "  detections_list = []\n",
        "  for annotation_doc in annotations:\n",
        "    for annotation in annotation_doc['annotations']:\n",
        "      path_to_img = f\"{images_folder}{annotation['filename']}\"\n",
        "      if not (path_to_img.endswith('.jpg') or path_to_img.endswith('.png')):\n",
        "        path_to_img = path_to_img + '.jpg'\n",
        "      im = cv2.imread(path_to_img)\n",
        "      start = time.time()\n",
        "      bounding_boxes = detector.detect(im)\n",
        "      inference_time = time.time() - start\n",
        "      detection = {\n",
        "          'filename': annotation['filename'],\n",
        "          'inference_time': inference_time,\n",
        "          'gt': annotation['faces'],\n",
        "          'pred': [{\n",
        "              'top_left': (float(box[0]), float(box[1])),\n",
        "              'bottom_right': (float(box[2]), float(box[3])),\n",
        "              'confidence': float(box[4])\n",
        "          } for box in bounding_boxes]\n",
        "      }\n",
        "      \n",
        "      detections_list.append(detection)\n",
        "    \n",
        "  return detections_list\n",
        "\n",
        "def evaluate_dsfd_and_write_to_file(annotations, images_folder):\n",
        "  algorithm = 'DSFD'\n",
        "  detections_list = dsfd_detector_eval(annotations, images_folder)\n",
        "\n",
        "  mean_time = np.mean([detection['inference_time'] for detection in detections_list])\n",
        "\n",
        "  evaluation = {\n",
        "      'dataset': DATASET,\n",
        "      'algorithm': algorithm, \n",
        "      'mean_time': mean_time,\n",
        "      'detections': detections_list\n",
        "  }\n",
        "\n",
        "  eval_file = open(f'{EVAL_FOLDER}Predictions of {algorithm} on {DATASET} dataset.json', 'w')\n",
        "  eval_file.write(json.dumps(evaluation))\n",
        "  eval_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XpjaAWh-2h0b"
      },
      "source": [
        "#@title YOLOFace detector - CPU\n",
        "CONF_THRESHOLD = 0.5\n",
        "NMS_THRESHOLD = 0.4\n",
        "IMG_WIDTH = 416\n",
        "IMG_HEIGHT = 416\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Help functions\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def refined_box(left, top, width, height):\n",
        "    right = left + width\n",
        "    bottom = top + height\n",
        "\n",
        "    original_vert_height = bottom - top\n",
        "    top = int(top + original_vert_height * 0.15)\n",
        "    bottom = int(bottom - original_vert_height * 0.05)\n",
        "\n",
        "    margin = ((bottom - top) - (right - left)) // 2\n",
        "    left = left - margin if (bottom - top - right + left) % 2 == 0 else left - margin - 1\n",
        "\n",
        "    right = right + margin\n",
        "\n",
        "    return left, top, right, bottom\n",
        "\n",
        "\n",
        "# Get the names of the output layers\n",
        "def get_outputs_names(net):\n",
        "    # Get the names of all the layers in the network\n",
        "    layers_names = net.getLayerNames()\n",
        "\n",
        "    # Get the names of the output layers, i.e. the layers with unconnected\n",
        "    # outputs\n",
        "    return [layers_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "\n",
        "def post_process(frame, outs, conf_threshold, nms_threshold):\n",
        "    frame_height = frame.shape[0]\n",
        "    frame_width = frame.shape[1]\n",
        "\n",
        "    # Scan through all the bounding boxes output from the network and keep only\n",
        "    # the ones with high confidence scores. Assign the box's class label as the\n",
        "    # class with the highest score.\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    final_boxes = []\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > conf_threshold:\n",
        "                center_x = int(detection[0] * frame_width)\n",
        "                center_y = int(detection[1] * frame_height)\n",
        "                width = int(detection[2] * frame_width)\n",
        "                height = int(detection[3] * frame_height)\n",
        "                left = int(center_x - width / 2)\n",
        "                top = int(center_y - height / 2)\n",
        "                confidences.append(float(confidence))\n",
        "                boxes.append([left, top, width, height])\n",
        "\n",
        "    # Perform non maximum suppression to eliminate redundant\n",
        "    # overlapping boxes with lower confidences.\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold,\n",
        "                               nms_threshold)\n",
        "\n",
        "    for i in indices:\n",
        "        i = i[0]\n",
        "        box = boxes[i]\n",
        "        left = box[0]\n",
        "        top = box[1]\n",
        "        width = box[2]\n",
        "        height = box[3]\n",
        "        left, top, right, bottom = refined_box(left, top, width, height)\n",
        "        box = {\n",
        "            'top_left': (float(left), float(top)),\n",
        "            'bottom_right': (float(right), float(bottom)),\n",
        "            'confidence': float(confidences[i])\n",
        "        }\n",
        "        final_boxes.append(box)\n",
        "\n",
        "    return final_boxes\n",
        "\n",
        "def yoloface_detector():\n",
        "  model_cfg = f\"{MODEL_FOLDER}cfg/yolov3-face.cfg\"\n",
        "  model_weights = f\"{MODEL_FOLDER}yolov3-wider_16000.weights\"\n",
        "  net = cv2.dnn.readNetFromDarknet(model_cfg, model_weights)\n",
        "  net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "  net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n",
        "  return net\n",
        "\n",
        "def run_yoloface(detector, image):\n",
        "  blob = cv2.dnn.blobFromImage(image, 1 / 255, (IMG_WIDTH, IMG_HEIGHT), [0, 0, 0], 1, crop=False)\n",
        "  detector.setInput(blob)\n",
        "  outs = detector.forward(get_outputs_names(detector))\n",
        "  faces = post_process(image, outs, CONF_THRESHOLD, NMS_THRESHOLD)\n",
        "  return faces\n",
        "\n",
        "\n",
        "# prepare detection and evaluation for YOLOFace\n",
        "def yoloface_detector_eval(annotations, images_folder):\n",
        "  # prepare detector\n",
        "  detector = yoloface_detector()\n",
        "  detections_list = []\n",
        "  for annotation_doc in annotations:\n",
        "    for annotation in annotation_doc['annotations']:\n",
        "      path_to_img = f\"{images_folder}{annotation['filename']}\"\n",
        "      if not (path_to_img.endswith('.jpg') or path_to_img.endswith('.png')):\n",
        "        path_to_img = path_to_img + '.jpg'\n",
        "      im = cv2.imread(path_to_img)\n",
        "      start = time.time()\n",
        "      bounding_boxes = run_yoloface(detector, im)\n",
        "      inference_time = time.time() - start\n",
        "      detection = {\n",
        "          'filename': annotation['filename'],\n",
        "          'inference_time': inference_time,\n",
        "          'gt': annotation['faces'],\n",
        "          'pred': bounding_boxes\n",
        "      }\n",
        "      detections_list.append(detection)\n",
        "  return detections_list\n",
        "\n",
        "def evaluate_yoloface_and_write_to_file(annotations, images_folder):\n",
        "  algorithm = 'YOLOFACE'\n",
        "  detections_list = yoloface_detector_eval(annotations, images_folder)\n",
        "  mean_time = np.mean([detection['inference_time'] for detection in detections_list])\n",
        "\n",
        "  evaluation = {\n",
        "      'dataset': DATASET,\n",
        "      'algorithm': algorithm,\n",
        "      'mean_time': mean_time,\n",
        "      'detections': detections_list\n",
        "  }\n",
        "  eval_file = open(f'{EVAL_FOLDER}Predictions of {algorithm} on {DATASET} dataset.json', 'w')\n",
        "  eval_file.write(json.dumps(evaluation))\n",
        "  eval_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TDO0aggFxOKD"
      },
      "source": [
        "#@title FaceBoxes pytorch Model\n",
        "import tensorflow.compat.v1 as tf1\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class FaceDetector:\n",
        "    def __init__(self, model_path, gpu_memory_fraction=0.25, visible_device_list='0'):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            model_path: a string, path to a pb file.\n",
        "            gpu_memory_fraction: a float number.\n",
        "            visible_device_list: a string.\n",
        "        \"\"\"\n",
        "        with tf1.io.gfile.GFile(model_path, 'rb') as f:\n",
        "            graph_def = tf1.GraphDef()\n",
        "            graph_def.ParseFromString(f.read())\n",
        "\n",
        "        graph = tf1.Graph()\n",
        "        with graph.as_default():\n",
        "            tf1.import_graph_def(graph_def, name='import')\n",
        "\n",
        "        self.input_image = graph.get_tensor_by_name('import/image_tensor:0')\n",
        "        self.output_ops = [\n",
        "            graph.get_tensor_by_name('import/boxes:0'),\n",
        "            graph.get_tensor_by_name('import/scores:0'),\n",
        "            graph.get_tensor_by_name('import/num_boxes:0'),\n",
        "        ]\n",
        "\n",
        "        gpu_options = tf1.GPUOptions(\n",
        "            per_process_gpu_memory_fraction=gpu_memory_fraction,\n",
        "            visible_device_list=visible_device_list\n",
        "        )\n",
        "        config_proto = tf1.ConfigProto(gpu_options=gpu_options, log_device_placement=False)\n",
        "        self.sess = tf1.Session(graph=graph, config=config_proto)\n",
        "\n",
        "    def __call__(self, image, score_threshold=0.5):\n",
        "        \"\"\"Detect faces.\n",
        "        Arguments:\n",
        "            image: a numpy uint8 array with shape [height, width, 3],\n",
        "                that represents a RGB image.\n",
        "            score_threshold: a float number.\n",
        "        Returns:\n",
        "            boxes: a float numpy array of shape [num_faces, 4].\n",
        "            scores: a float numpy array of shape [num_faces].\n",
        "        Note that box coordinates are in the order: ymin, xmin, ymax, xmax!\n",
        "        \"\"\"\n",
        "        h, w, _ = image.shape\n",
        "        image = np.expand_dims(image, 0)\n",
        "\n",
        "        boxes, scores, num_boxes = self.sess.run(\n",
        "            self.output_ops, feed_dict={self.input_image: image}\n",
        "        )\n",
        "        num_boxes = num_boxes[0]\n",
        "        boxes = boxes[0][:num_boxes]\n",
        "        scores = scores[0][:num_boxes]\n",
        "\n",
        "        to_keep = scores > score_threshold\n",
        "        boxes = boxes[to_keep]\n",
        "        scores = scores[to_keep]\n",
        "\n",
        "        scaler = np.array([h, w, h, w], dtype='float32')\n",
        "        boxes = boxes * scaler\n",
        "\n",
        "        return boxes, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wC4uRx_wBYox"
      },
      "source": [
        "#@title FaceBoxes Detector\n",
        "%tensorflow_version 1.x\n",
        "# prepare detection and evaluation for FaceBoxes\n",
        "def faceboxes_detector_eval(annotations, images_folder):\n",
        "  # prepare detector\n",
        "  FACEBOXES_MODEL = f\"{MODEL_FOLDER}faceboxes.pb\"\n",
        "  detector = FaceDetector(FACEBOXES_MODEL)\n",
        "  detections_list = []\n",
        "  for annotation_doc in annotations:\n",
        "    for annotation in annotation_doc['annotations']:\n",
        "      path_to_img = f\"{images_folder}{annotation['filename']}\"\n",
        "      if not (path_to_img.endswith('.jpg') or path_to_img.endswith('.png')):\n",
        "        path_to_img = path_to_img + '.jpg'\n",
        "      im = cv2.imread(path_to_img)\n",
        "      im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "      start = time.time()\n",
        "      bounding_boxes, scores = detector(im)\n",
        "      inference_time = time.time() - start\n",
        "      detection = {\n",
        "          'filename': annotation['filename'],\n",
        "          'inference_time': inference_time,\n",
        "          'gt': annotation['faces'],\n",
        "          'pred': [{\n",
        "              'top_left': (float(box[1]), float(box[0])),\n",
        "              'bottom_right': (float(box[3]), float(box[2])),\n",
        "              'confidence': float(score)\n",
        "          } for box, score in zip(bounding_boxes, scores)]\n",
        "      }\n",
        "      detections_list.append(detection)\n",
        "\n",
        "  return detections_list\n",
        "\n",
        "def evaluate_faceboxes_and_write_to_file(annotations, images_folder):\n",
        "  algorithm = 'FaceBoxes'\n",
        "  detections_list = faceboxes_detector_eval(annotations, images_folder)\n",
        "\n",
        "  mean_time = np.mean([detection['inference_time'] for detection in detections_list])\n",
        "\n",
        "  evaluation = {\n",
        "      'dataset': DATASET,\n",
        "      'algorithm': algorithm, \n",
        "      'mean_time': mean_time,\n",
        "      'detections': detections_list\n",
        "  }\n",
        "\n",
        "  eval_file = open(f'{EVAL_FOLDER}Predictions of {algorithm} on {DATASET} dataset.json', 'w')\n",
        "  eval_file.write(json.dumps(evaluation))\n",
        "  eval_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "I0qJqu3mn4Yv"
      },
      "source": [
        "#@title Retina Face import\n",
        "# install retinaface\n",
        "!pip install retinaface-pytorch\n",
        "# Retinaface example\n",
        "from retinaface.pre_trained_models import get_model\n",
        "\n",
        "def retinaface_example_test(path_to_img):\n",
        "  model = get_model(\"resnet50_2020-07-20\", max_size=2048)\n",
        "  model.eval()\n",
        "  im = cv2.imread(path_to_img)\n",
        "  start = time.time()\n",
        "  detections = model.predict_jsons(im, confidence_threshold=.5, nms_threshold=0.3)\n",
        "  print(\"time: \", time.time() - start)\n",
        "  for rect in detections:\n",
        "    rectangle = rect['bbox']\n",
        "    cv2.rectangle(im, (int(rectangle[0]), int(rectangle[1])), (int(rectangle[2]), int(rectangle[3])), (0,0,190), 2)\n",
        "  cv2_imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8b2V-w8XomRU"
      },
      "source": [
        "#@title Retina Face Super slow face detector !\n",
        "retinaface_example_test(\"/content/drive/MyDrive/Datasets/FaceDetection/WIDERFace/test/images/22--Picnic/22_Picnic_Picnic_22_106.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "4m5p67ENEPFT"
      },
      "source": [
        "#@title Pascal Face problem\n",
        "img = '/content/drive/MyDrive/Datasets/FaceDetection/PascalFace/test/images/image_0242.png'\n",
        "\n",
        "real = { \n",
        "    \"top_left\": (355, 57), \n",
        "    \"bottom_right\": (712, 554)\n",
        "    }\n",
        "\n",
        "pred = { \n",
        "    \"top_left\": (415, 216), \n",
        "    \"bottom_right\": (636, 437)\n",
        "    }\n",
        "\n",
        "im = cv2.imread(img)\n",
        "\n",
        "cv2.rectangle(im, real['top_left'], real['bottom_right'], (0,255,0))\n",
        "cv2.rectangle(im, pred['top_left'], pred['bottom_right'], (0,0,255))\n",
        "cv2_imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1BmIRUMOkn"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Uw4mspgw0wsM"
      },
      "source": [
        "#@title Load Dataset annotations and filenames\n",
        "#default: load rectangles list\n",
        "def load_annotations(path, atype='rectangle'):\n",
        "  filenames = []\n",
        "  annotations = []\n",
        "  for filename in os.listdir(path):\n",
        "    if filename.__contains__(atype):\n",
        "      filenames.append(filename)\n",
        "  for filename in filenames:\n",
        "    raw = open(path + filename)\n",
        "    doc = {\n",
        "        'annotations_filename': filename,\n",
        "        'annotations': json.loads(raw.read())\n",
        "    }\n",
        "    annotations.append(doc)\n",
        "    raw.close()\n",
        "  return annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2PkdVI3pv-n"
      },
      "source": [
        "**Load WiderFace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yDbvL7cVp0PX"
      },
      "source": [
        "#@title Wider Face loading\n",
        "DATASET = \"WIDERFace\"\n",
        "##########################################################################\n",
        "DATASET_PATH = f\"{ROOT}{DATASET}/\"\n",
        "train_path = f\"{DATASET_PATH}train/\"\n",
        "test_path = f\"{DATASET_PATH}test/\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJIUO9GEp0ki"
      },
      "source": [
        "**Load FDDB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PIfO-Oimp3Sv"
      },
      "source": [
        "#@title FDDB loading\n",
        "DATASET = \"FDDB\"\n",
        "##########################################################################\n",
        "DATASET_PATH = f\"{ROOT}{DATASET}/\"\n",
        "train_path = f\"{DATASET_PATH}train/\"\n",
        "test_path = f\"{DATASET_PATH}test/\"\n",
        "annotations_path = f\"{test_path}annotations/\"\n",
        "annotations = load_annotations(annotations_path)\n",
        "images_folder = test_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enpD_katp3gy"
      },
      "source": [
        "**Load Pascal Face**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Hd5KJMYeqA69"
      },
      "source": [
        "#@title Pascal Face loading\n",
        "DATASET = \"PascalFace\"\n",
        "##########################################################################\n",
        "DATASET_PATH = f\"{ROOT}{DATASET}/\"\n",
        "train_path = f\"{DATASET_PATH}train/\"\n",
        "test_path = f\"{DATASET_PATH}test/\"\n",
        "annotations_path = f\"{test_path}annotations/\"\n",
        "annotations = load_annotations(annotations_path)\n",
        "images_folder = f\"{test_path}images/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovg83Ml1yMvc"
      },
      "source": [
        "# Test Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L43hlvwdds7k"
      },
      "source": [
        "**DSFD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj0evdTJA-rE"
      },
      "source": [
        "# run evaluation\n",
        "evaluate_dsfd_and_write_to_file(annotations, images_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COrkUVlnRbve"
      },
      "source": [
        "**YOLOFace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hktwCgcAQnNb"
      },
      "source": [
        "# run evaluation\n",
        "evaluate_yoloface_and_write_to_file(annotations, images_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EUjKJb6DfDH"
      },
      "source": [
        "**FaceBoxes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h265W4BtDekp"
      },
      "source": [
        "evaluate_faceboxes_and_write_to_file(annotations, images_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "HrjbSjU5DqBT"
      },
      "source": [
        "#@title Yolo Detector using GPU (or TPU) { vertical-output: true }\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()\n",
        "\n",
        "\n",
        "def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n",
        "    '''Convert final layer features to bounding box parameters'''\n",
        "\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    # Reshape to batch, height, width, num_anchors, box_params.\n",
        "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
        "\n",
        "    # height, width\n",
        "    grid_shape = K.shape(feats)[1:3]\n",
        "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n",
        "                    [1, grid_shape[1], 1, 1])\n",
        "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n",
        "                    [grid_shape[0], 1, 1, 1])\n",
        "    grid = K.concatenate([grid_x, grid_y])\n",
        "    grid = K.cast(grid, K.dtype(feats))\n",
        "\n",
        "    feats = K.reshape(\n",
        "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
        "\n",
        "    # Adjust preditions to each spatial grid point and anchor size.\n",
        "    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1],\n",
        "                                                         K.dtype(feats))\n",
        "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1],\n",
        "                                                              K.dtype(feats))\n",
        "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
        "    box_class_probs = K.sigmoid(feats[..., 5:])\n",
        "\n",
        "    if calc_loss == True:\n",
        "        return grid, feats, box_xy, box_wh\n",
        "    return box_xy, box_wh, box_confidence, box_class_probs\n",
        "\n",
        "\n",
        "def correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
        "    '''Get corrected boxes'''\n",
        "\n",
        "    box_yx = box_xy[..., ::-1]\n",
        "    box_hw = box_wh[..., ::-1]\n",
        "    input_shape = K.cast(input_shape, K.dtype(box_yx))\n",
        "    image_shape = K.cast(image_shape, K.dtype(box_yx))\n",
        "    new_shape = K.round(image_shape * K.min(input_shape / image_shape))\n",
        "    offset = (input_shape - new_shape) / 2. / input_shape\n",
        "    scale = input_shape / new_shape\n",
        "    box_yx = (box_yx - offset) * scale\n",
        "    box_hw *= scale\n",
        "\n",
        "    box_mins = box_yx - (box_hw / 2.)\n",
        "    box_maxes = box_yx + (box_hw / 2.)\n",
        "    boxes = K.concatenate([\n",
        "        box_mins[..., 0:1],  # y_min\n",
        "        box_mins[..., 1:2],  # x_min\n",
        "        box_maxes[..., 0:1],  # y_max\n",
        "        box_maxes[..., 1:2]  # x_max\n",
        "    ])\n",
        "\n",
        "    # Scale boxes back to original image shape.\n",
        "    boxes *= K.concatenate([image_shape, image_shape])\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def boxes_and_scores(feats, anchors, num_classes, input_shape,\n",
        "                     image_shape):\n",
        "    '''Process Convolutional layer output'''\n",
        "\n",
        "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n",
        "                                                                anchors,\n",
        "                                                                num_classes,\n",
        "                                                                input_shape)\n",
        "    boxes = correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
        "    boxes = K.reshape(boxes, [-1, 4])\n",
        "    box_scores = box_confidence * box_class_probs\n",
        "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
        "    return boxes, box_scores\n",
        "\n",
        "\n",
        "def eval(outputs, anchors, num_classes, image_shape,\n",
        "         max_boxes=20, score_threshold=.6, iou_threshold=.5):\n",
        "    '''Evaluate the YOLO model on given input and return filtered boxes'''\n",
        "\n",
        "    num_layers = len(outputs)\n",
        "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] if num_layers == 3 else [\n",
        "        [3, 4, 5], [1, 2, 3]]\n",
        "    input_shape = K.shape(outputs[0])[1:3] * 32\n",
        "    boxes = []\n",
        "    box_scores = []\n",
        "\n",
        "    for l in range(num_layers):\n",
        "        _boxes, _box_scores = boxes_and_scores(outputs[l],\n",
        "                                               anchors[anchor_mask[l]],\n",
        "                                               num_classes, input_shape,\n",
        "                                               image_shape)\n",
        "        boxes.append(_boxes)\n",
        "        box_scores.append(_box_scores)\n",
        "\n",
        "    boxes = K.concatenate(boxes, axis=0)\n",
        "    box_scores = K.concatenate(box_scores, axis=0)\n",
        "\n",
        "    mask = box_scores >= score_threshold\n",
        "    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n",
        "    boxes_ = []\n",
        "    scores_ = []\n",
        "    classes_ = []\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        # TODO: use Keras backend instead of tf.\n",
        "        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n",
        "        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n",
        "        nms_index = tf.image.non_max_suppression(\n",
        "            class_boxes, class_box_scores, max_boxes_tensor,\n",
        "            iou_threshold=iou_threshold)\n",
        "        class_boxes = K.gather(class_boxes, nms_index)\n",
        "        class_box_scores = K.gather(class_box_scores, nms_index)\n",
        "        classes = K.ones_like(class_box_scores, 'int32') * c\n",
        "        boxes_.append(class_boxes)\n",
        "        scores_.append(class_box_scores)\n",
        "        classes_.append(classes)\n",
        "\n",
        "    boxes_ = K.concatenate(boxes_, axis=0)\n",
        "    scores_ = K.concatenate(scores_, axis=0)\n",
        "    classes_ = K.concatenate(classes_, axis=0)\n",
        "\n",
        "    return boxes_, scores_, classes_\n",
        "\n",
        "\n",
        "class YOLO(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.model_path = args.model\n",
        "        self.classes_path = args.classes\n",
        "        self.anchors_path = args.anchors\n",
        "        self.class_names = self._get_class()\n",
        "        self.anchors = self._get_anchors()\n",
        "        self.sess = K.get_session()\n",
        "        self.boxes, self.scores, self.classes = self._generate()\n",
        "        self.model_image_size = args.img_size\n",
        "\n",
        "    def _get_class(self):\n",
        "        class_names = ['face']\n",
        "        return class_names\n",
        "\n",
        "    def _get_anchors(self):\n",
        "        anchors_path = os.path.expanduser(self.anchors_path)\n",
        "        with open(anchors_path) as f:\n",
        "            anchors = f.readline()\n",
        "        anchors = [float(x) for x in anchors.split(',')]\n",
        "        return np.array(anchors).reshape(-1, 2)\n",
        "\n",
        "    def _generate(self):\n",
        "        model_path = os.path.expanduser(self.model_path)\n",
        "        assert model_path.endswith(\n",
        "            '.h5'), 'Keras model or weights must be a .h5 file'\n",
        "\n",
        "        # load model, or construct model and load weights\n",
        "        num_anchors = len(self.anchors)\n",
        "        num_classes = len(self.class_names)\n",
        "        try:\n",
        "            self.yolo_model = load_model(model_path, compile=False)\n",
        "        except:\n",
        "            # make sure model, anchors and classes match\n",
        "            self.yolo_model.load_weights(self.model_path)\n",
        "        else:\n",
        "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
        "                   num_anchors / len(self.yolo_model.output) * (\n",
        "                           num_classes + 5), \\\n",
        "                'Mismatch between model and given anchor and class sizes'\n",
        "        print(\n",
        "            '*** {} model, anchors, and classes loaded.'.format(model_path))\n",
        "\n",
        "        # generate output tensor targets for filtered bounding boxes.\n",
        "        self.input_image_shape = K.placeholder(shape=(2,))\n",
        "        boxes, scores, classes = eval(self.yolo_model.output, self.anchors,\n",
        "                                           len(self.class_names),\n",
        "                                           self.input_image_shape,\n",
        "                                           score_threshold=self.args.score,\n",
        "                                           iou_threshold=self.args.iou)\n",
        "        return boxes, scores, classes\n",
        "\n",
        "    def detect_image(self, image):\n",
        "        start_time = timer()\n",
        "        if self.model_image_size != (None, None):\n",
        "            assert self.model_image_size[\n",
        "                       0] % 32 == 0, 'Multiples of 32 required'\n",
        "            assert self.model_image_size[\n",
        "                       1] % 32 == 0, 'Multiples of 32 required'\n",
        "            boxed_image = letterbox_image(image, tuple(\n",
        "                reversed(self.model_image_size)))\n",
        "        else:\n",
        "            new_image_size = (image.width - (image.width % 32),\n",
        "                              image.height - (image.height % 32))\n",
        "            boxed_image = letterbox_image(image, new_image_size)\n",
        "        image_data = np.array(boxed_image, dtype='float32')\n",
        "        image_data /= 255.\n",
        "        # add batch dimension\n",
        "        image_data = np.expand_dims(image_data, 0)\n",
        "        out_boxes, out_scores, out_classes = self.sess.run(\n",
        "            [self.boxes, self.scores, self.classes],\n",
        "            feed_dict={\n",
        "                self.yolo_model.input: image_data,\n",
        "                self.input_image_shape: [image.size[1], image.size[0]],\n",
        "                K.learning_phase(): 0\n",
        "            })\n",
        "        print('*** Found {} face(s) for this image'.format(len(out_boxes)))\n",
        "        end_time = timer()\n",
        "        print((end_time - start_time) * 1000)\n",
        "        for i, c in reversed(list(enumerate(out_classes))):\n",
        "            box = out_boxes[i]\n",
        "            score = out_scores[i]\n",
        "            top, left, bottom, right = box\n",
        "\n",
        "        return out_boxes\n",
        "\n",
        "    def close_session(self):\n",
        "        self.sess.close()\n",
        "\n",
        "\n",
        "def letterbox_image(image, size):\n",
        "    '''Resize image with unchanged aspect ratio using padding'''\n",
        "\n",
        "    img_width, img_height = image.size\n",
        "    w, h = size\n",
        "    scale = min(w / img_width, h / img_height)\n",
        "    nw = int(img_width * scale)\n",
        "    nh = int(img_height * scale)\n",
        "\n",
        "    image = image.resize((nw, nh), Image.BICUBIC)\n",
        "    new_image = Image.new('RGB', size, (128, 128, 128))\n",
        "    new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
        "    return new_image\n",
        "\n",
        "\n",
        "def detect_img(yolo, img):\n",
        "    image = Image.open(img)\n",
        "    bboxes = yolo.detect_image(image)\n",
        "    print(bboxes)\n",
        "    #yolo.close_session()\n",
        "\n",
        "class Args():\n",
        "    def __init__(self, model=\"/content/drive/MyDrive/Models/FaceDetection/YOLO_Face.h5\", anchors='cfg/yolo_anchors.txt', classes='cfg/face_classes.txt', score=0.5, iou=0.45, img_size=(416,416), output='outputs/'):\n",
        "        self.model = model\n",
        "        self.anchors = anchors\n",
        "        self.classes = classes\n",
        "        self.score = score\n",
        "        self.iou = iou\n",
        "        self.img_size = img_size\n",
        "        self.output = output\n",
        "\n",
        "args = Args()\n",
        "img_path = \"/content/drive/MyDrive/Datasets/FaceDetection/WIDERFace/test/images/22--Picnic/22_Picnic_Picnic_22_106.jpg\"\n",
        "yolo = YOLO(args)\n",
        "detect_img(yolo, img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pa4UNg9W9TH"
      },
      "source": [
        "# TinaFace test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "zMXH9yGFZCis"
      },
      "source": [
        "#@title Installing Vedadet\n",
        "!git clone https://github.com/Media-Smart/vedadet.git\n",
        "# move vedadet folder to your drive\n",
        "VEDANET_ROOT = '/content/drive/MyDrive/Models/FaceDetection/vedadet/'\n",
        "!pip install -v -e $VEDANET_ROOT\n",
        "\n",
        " infer = '/content/drive/MyDrive/Models/FaceDetection/vedadet/tools/infer.py'\n",
        " configs = '/content/drive/MyDrive/Models/FaceDetection/vedadet/configs/infer/tinaface/tinaface.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAVnFDswtSRt"
      },
      "source": [
        "infer = '/content/drive/MyDrive/Models/FaceDetection/vedadet/tools/infer.py'\n",
        "configs = '/content/drive/MyDrive/Models/FaceDetection/vedadet/configs/infer/tinaface/tinaface.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6LihPQIcphW"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=\"0\" python $infer $configs $annotations_path $images_folder $DATASET"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}